{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2d99eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Brain (Ollama)...\n",
      "üîå Connecting to Memory (ChromaDB)...\n",
      "   ‚ö† WARNING: 'chroma_db' folder not found. Running in 'Amnesia Mode' (No RAG).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\IPython\\core\\formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langgraph\\pregel\\main.py:775\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    772\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    774\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    776\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:693\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config, base_url, proxies)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    684\u001b[39m     draw_mermaid_png,\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    688\u001b[39m     curve_style=curve_style,\n\u001b[32m    689\u001b[39m     node_colors=node_colors,\n\u001b[32m    690\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    691\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    692\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:312\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, base_url, proxies)\u001b[39m\n\u001b[32m    306\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    307\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    308\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    309\u001b[39m         )\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:475\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay, proxies, base_url)\u001b[39m\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    471\u001b[39m     msg = (\n\u001b[32m    472\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    473\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    479\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x16d409beb10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Import M3's Persona\n",
    "from prompts import SOMY_SYSTEM_PROMPT, CRISIS_RESPONSE\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DB_PATH = \"chroma_db\" # This folder must exist (From M2)\n",
    "LLM_MODEL = \"llama3.1\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "# --- 1. SETUP MODELS ---\n",
    "print(\"üîå Connecting to Brain (Ollama)...\")\n",
    "try:\n",
    "    llm = ChatOllama(model=LLM_MODEL, temperature=0.7)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "    llm = None\n",
    "\n",
    "print(\"üîå Connecting to Memory (ChromaDB)...\")\n",
    "# We connect to the DB created by M2. If it doesn't exist, we skip RAG.\n",
    "retriever = None\n",
    "if os.path.exists(DB_PATH):\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "        vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) # Get top 3 pages\n",
    "        print(\"   ‚úÖ Database connected.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Database error: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ö† WARNING: 'chroma_db' folder not found. Running in 'Amnesia Mode' (No RAG).\")\n",
    "\n",
    "# --- 2. DEFINE STATE ---\n",
    "# This dictionary holds the data as it moves through the graph nodes\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[object] # Chat history\n",
    "    context: str           # Retrieved book info\n",
    "    is_safe: bool          # Safety flag\n",
    "\n",
    "# --- 3. DEFINE NODES (The Workers) ---\n",
    "\n",
    "def safety_check(state: AgentState):\n",
    "    \"\"\"Worker 1: The Guardrail (M4 Logic)\"\"\"\n",
    "    # Get the last message from the user\n",
    "    last_msg = state[\"messages\"][-1].content.lower()\n",
    "    danger_words = [\"die\", \"kill\", \"suicide\", \"end it\", \"hurt myself\"]\n",
    "    \n",
    "    # Check if safe\n",
    "    is_unsafe = any(word in last_msg for word in danger_words)\n",
    "    \n",
    "    if is_unsafe:\n",
    "        print(\"üö® GUARDRAIL HIT: Unsafe content detected.\")\n",
    "        \n",
    "    return {\"is_safe\": not is_unsafe}\n",
    "\n",
    "def retrieve_knowledge(state: AgentState):\n",
    "    \"\"\"Worker 2: The Librarian (M2 Logic)\"\"\"\n",
    "    if not retriever:\n",
    "        return {\"context\": \"No medical manuals available.\"}\n",
    "    \n",
    "    print(\"üîç Searching Knowledge Base...\")\n",
    "    query = state[\"messages\"][-1].content\n",
    "    try:\n",
    "        docs = retriever.invoke(query)\n",
    "        # Combine the top 3 chunks into one string\n",
    "        context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        return {\"context\": context_text}\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Retrieval Failed: {e}\")\n",
    "        return {\"context\": \"Error retrieving context.\"}\n",
    "\n",
    "def generate_response(state: AgentState):\n",
    "    \"\"\"Worker 3: The Psychologist (M3 Logic)\"\"\"\n",
    "    \n",
    "    # If safety check failed, return crisis message immediately\n",
    "    if not state[\"is_safe\"]:\n",
    "        return {\"messages\": [AIMessage(content=CRISIS_RESPONSE)]}\n",
    "    \n",
    "    if not llm:\n",
    "        return {\"messages\": [AIMessage(content=\"System Error: AI Model not connected.\")]}\n",
    "\n",
    "    print(\"ü§ñ Generating Response...\")\n",
    "    \n",
    "    # Inject the retrieved context into the System Prompt\n",
    "    # If context is empty, it just uses general knowledge\n",
    "    filled_prompt = SOMY_SYSTEM_PROMPT.format(context=state.get(\"context\", \"No manuals.\"))\n",
    "    \n",
    "    # Build the conversation history for the LLM:\n",
    "    # [System Instruction] + [User Message]\n",
    "    conversation = [SystemMessage(content=filled_prompt)] + state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(conversation)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- 4. BUILD THE GRAPH ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add the workers to the graph\n",
    "workflow.add_node(\"safety\", safety_check)\n",
    "workflow.add_node(\"retrieve\", retrieve_knowledge)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "\n",
    "# Define the Flow: Start -> Safety\n",
    "workflow.set_entry_point(\"safety\")\n",
    "\n",
    "# Define Conditional Flow (The \"Traffic Cop\")\n",
    "def route_safety(state: AgentState) -> Literal[\"retrieve\", \"generate\"]:\n",
    "    if state[\"is_safe\"]:\n",
    "        return \"retrieve\" # If safe, go get knowledge\n",
    "    else:\n",
    "        return \"generate\" # If unsafe, skip to generation (which will output crisis msg)\n",
    "\n",
    "workflow.add_conditional_edges(\"safety\", route_safety)\n",
    "\n",
    "# Normal Flow\n",
    "workflow.add_edge(\"retrieve\", END)\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the machine\n",
    "workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac16a0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project - Copy\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Brain (Ollama)...\n",
      "üîå Connecting to Memory (pinecone)...\n",
      "   ‚úÖ Connected to Pinecone Index.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore # <-- Added Pinecone\n",
    "from ai_router import router_node\n",
    "\n",
    "# Import M3's Persona\n",
    "from prompts import SOMY_SYSTEM_PROMPT, CRISIS_RESPONSE\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PINECONE_API_KEY = \"pcsk_2YdHFw_67VFCkBgYApLCgajHUvuQEcgvjp2j6hX9sCetXd8RcLc2xumsEokE4zUpSSag8C\" # <-- New Config (M1 needs this from M2)\n",
    "INDEX_NAME = \"somy-ali-brain\"\n",
    "LLM_MODEL = \"llama3.1\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "# --- 1. SETUP MODELS ---\n",
    "print(\"üîå Connecting to Brain (Ollama)...\")\n",
    "try:\n",
    "    llm = ChatOllama(model=LLM_MODEL, temperature=0.7)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "    llm = None\n",
    "\n",
    "print(\"üîå Connecting to Memory (pinecone)...\")\n",
    "# We connect to the DB created by M2. If it doesn't exist, we skip RAG.\n",
    "retriever = None\n",
    "try:\n",
    "    # Initialize Pinecone Client\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    \n",
    "    # Initialize Embeddings\n",
    "    embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "    \n",
    "    # Connect to the Index\n",
    "    vector_store = PineconeVectorStore(\n",
    "        index_name=INDEX_NAME,\n",
    "        embedding=embeddings,\n",
    "        pinecone_api_key=PINECONE_API_KEY\n",
    "    )\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    print(\"   ‚úÖ Connected to Pinecone Index.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Pinecone Error: {e}\")\n",
    "    print(\"   Running in 'Amnesia Mode' (No RAG).\")\n",
    "\n",
    "# --- 2. DEFINE STATE ---\n",
    "# This dictionary holds the data as it moves through the graph nodes\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[object] # Chat history\n",
    "    context: str           # Retrieved book info\n",
    "    is_safe: bool          # Safety flag\n",
    "    filter_category: str  # From Router\n",
    "\n",
    "# --- 3. DEFINE NODES (The Workers) ---\n",
    "\n",
    "def safety_check(state: AgentState):\n",
    "    \"\"\"Worker 1: The Guardrail (M4 Logic)\"\"\"\n",
    "    # Get the last message from the user\n",
    "    # We check if messages list is not empty\n",
    "    if not state[\"messages\"]:\n",
    "        return {\"is_safe\": True}\n",
    "    \n",
    "    last_msg = state[\"messages\"][-1].content.lower()\n",
    "    danger_words = [\n",
    "    # Core suicide terms\n",
    "    \"suicide\",\n",
    "    \"suicidal\",\n",
    "    \"kill myself\",\n",
    "    \"end my life\",\n",
    "    \"take my own life\",\n",
    "    \"want to die\",\n",
    "    \"wish I were dead\",\n",
    "    \"death is the only solution\",\n",
    "    \"self-harm\",\n",
    "    \"self harm\",\n",
    "    \"hurt myself\",\n",
    "    \"harm myself\",\n",
    "    \"cut myself\",\n",
    "    \"cutting\",\n",
    "    \"bleed to death\",\n",
    "    \"cut my veins\",\n",
    "    \"smash my head\",\n",
    "    \"punish myself\",\n",
    "    \"overdose\",\n",
    "    \"poison myself\",\n",
    "    \"jump off\",\n",
    "    \"jump in front of a train\",\n",
    "    \"drown myself\",\n",
    "    \"suffocate myself\",\n",
    "    \"end it all\",\n",
    "    \"end the pain\",\n",
    "    \"end my suffering\",\n",
    "    \"stop existing\",\n",
    "    \"disappear forever\",\n",
    "    \"vanish from this world\",\n",
    "    \"escape life\",\n",
    "    \"free from life\",\n",
    "    \"life is meaningless\",\n",
    "    \"life is not worth living\",\n",
    "    \"no reason to live\",\n",
    "    \"no future for me\",\n",
    "    \"tired of being alive\",\n",
    "    \"done with life\",\n",
    "    \"giving up on life\",\n",
    "    \"I am a burden\",\n",
    "    \"everyone would be better without me\",\n",
    "    \"no one would miss me\",\n",
    "    \"tonight\",\n",
    "    \"right now\",\n",
    "    \"anymore\"\n",
    "]\n",
    "    \n",
    "    # Check if safe\n",
    "    is_unsafe = any(word in last_msg for word in danger_words)\n",
    "    \n",
    "    if is_unsafe:\n",
    "        print(\"üö® GUARDRAIL HIT: Unsafe content detected.\")\n",
    "        \n",
    "    return {\"is_safe\": not is_unsafe}\n",
    "\n",
    "def retrieve_knowledge(state: AgentState):\n",
    "    \"\"\"Worker 2: The Smart Librarian (Updated for Week 3)\"\"\"\n",
    "    if not retriever:\n",
    "        return {\"context\": \"No medical manuals available.\"}\n",
    "    \n",
    "    print(\"üîç Searching Knowledge Base...\")\n",
    "    user_text = state[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    # --- METADATA ROUTING LOGIC ---\n",
    "    # We check the user's text for keywords to decide which \"Book\" to open.\n",
    "    # This matches the tags M2 created in 'ingest_with_tags.py'.\n",
    "    \n",
    "    target_category = state.get(\"filter_category\", \"general\")\n",
    "    filter_dict = None # Default: Search everything\n",
    "    \n",
    "    # if \"child\" in user_text or \"teen\" in user_text or \"son\" in user_text or \"daughter\" in user_text:\n",
    "    #     print(\"   üè∑Ô∏è Filter Applied: Audience='child'\")\n",
    "    #     filter_dict = {\"audience\": \"child\"}\n",
    "        \n",
    "    # elif \"panic\" in user_text or \"anxiety\" in user_text or \"worry\" in user_text:\n",
    "    #     print(\"   üè∑Ô∏è Filter Applied: Category='anxiety'\")\n",
    "    #     filter_dict = {\"category\": \"anxiety\"}\n",
    "        \n",
    "    # elif \"sad\" in user_text or \"depress\" in user_text or \"hopeless\" in user_text:\n",
    "    #     print(\"   üè∑Ô∏è Filter Applied: Category='depression'\")\n",
    "    #     filter_dict = {\"category\": \"depression\"}\n",
    "\n",
    "    # elif \"trauma\" in user_text or \"ptsd\" in user_text or \"abuse\" in user_text:\n",
    "    #     print(\"   üè∑Ô∏è Filter Applied: Category='trauma'\")\n",
    "    #     filter_dict = {\"category\": \"trauma\"}\n",
    "    # elif \"anger\" in user_text:\n",
    "    #     filter_dict = {\"category\": \"anger\"}\n",
    "    # elif \"grief\" in user_text:\n",
    "    #     filter_dict = {\"category\": \"grief\"}\n",
    "    # elif \"sleep\" in user_text:\n",
    "    #     filter_dict = {\"category\": \"sleep\"}\n",
    "    # elif \"addiction\" in user_text or \"substance\" in user_text:\n",
    "    #     filter_dict = {\"category\": \"addiction\"}\n",
    "#-------Router which replace the above keyword matching logic-------#\n",
    "\n",
    "    if target_category == \"child\":\n",
    "        print(\"   üè∑Ô∏è Filter: Audience='child'\")\n",
    "        filter_dict = {\"audience\": \"child\"}\n",
    "    elif target_category != \"general\":\n",
    "        print(f\"   üè∑Ô∏è Filter: Category='{target_category}' OR 'general'\")\n",
    "        filter_dict = {\n",
    "            \"$or\": [\n",
    "                {\"category\": target_category},\n",
    "                {\"category\": \"general\"} \n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        print(\"   üè∑Ô∏è No specific filter (General).\")\n",
    "        \n",
    "        \n",
    "    # --- EXECUTE SEARCH ---\n",
    "    try:\n",
    "        # We pass the 'filter' to the retriever\n",
    "        if filter_dict:\n",
    "            docs = retriever.invoke(user_text, filter=filter_dict)\n",
    "        else:\n",
    "            docs = retriever.invoke(user_text) # Search everything if no keyword found\n",
    "            \n",
    "        # Combine results\n",
    "        if not docs:\n",
    "            return {\"context\": \"No specific manual found for this topic.\"}\n",
    "            \n",
    "        context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        return {\"context\": context_text}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Retrieval Failed: {e}\")\n",
    "        return {\"context\": \"Error retrieving context.\"}\n",
    "\n",
    "# def retrieve_knowledge(state: AgentState):\n",
    "#     \"\"\"Worker 2: The Librarian (M2 Logic)\"\"\"\n",
    "#     if not retriever:\n",
    "#         return {\"context\": \"No medical manuals available.\"}\n",
    "    \n",
    "#     print(\"üîç Searching Knowledge Base...\")\n",
    "#     query = state[\"messages\"][-1].content\n",
    "#     try:\n",
    "#         docs = retriever.invoke(query)\n",
    "#         # Combine the top 3 chunks into one string\n",
    "#         context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "#         return {\"context\": context_text}\n",
    "#     except Exception as e:\n",
    "#         print(f\"   ‚ö† Retrieval Failed: {e}\")\n",
    "#         return {\"context\": \"Error retrieving context.\"}\n",
    "\n",
    "def generate_response(state: AgentState):\n",
    "    \"\"\"Worker 3: The Psychologist (M3 Logic)\"\"\"\n",
    "    \n",
    "    # If safety check failed, return crisis message immediately\n",
    "    if not state[\"is_safe\"]:\n",
    "        return {\"messages\": [AIMessage(content=CRISIS_RESPONSE)]}\n",
    "    \n",
    "    if not llm:\n",
    "        return {\"messages\": [AIMessage(content=\"System Error: AI Model not connected.\")]}\n",
    "\n",
    "    print(\"ü§ñ Generating Response...\")\n",
    "    \n",
    "    # Inject the retrieved context into the System Prompt\n",
    "    # If context is empty, it just uses general knowledge\n",
    "    filled_prompt = SOMY_SYSTEM_PROMPT.format(context=state.get(\"context\", \"No manuals.\"))\n",
    "    \n",
    "    # Build the conversation history for the LLM:\n",
    "    # We create a new list for the LLM call to ensure structure is correct\n",
    "    conversation_for_llm = [SystemMessage(content=filled_prompt)] + state[\"messages\"]\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(conversation_for_llm)\n",
    "        return {\"messages\": [response]}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error invoking LLM: {e}\")\n",
    "        return {\"messages\": [AIMessage(content=\"I am having trouble finding the right words. Can you say that again?\")]}\n",
    "\n",
    "# --- 4. BUILD THE GRAPH ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add the workers to the graph\n",
    "workflow.add_node(\"safety\", safety_check)\n",
    "workflow.add_node(\"router\", router_node)\n",
    "workflow.add_node(\"retrieve\", retrieve_knowledge)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "\n",
    "# Define the Flow: Start -> Safety\n",
    "workflow.set_entry_point(\"safety\")\n",
    "\n",
    "# Define Conditional Flow (The \"Traffic Cop\")\n",
    "def route_safety(state: AgentState) -> Literal[\"router\", \"generate\"]:\n",
    "    if state[\"is_safe\"]:\n",
    "        return \"router\" # If safe, go get knowledge -- gp through router\n",
    "    else:\n",
    "        return \"generate\" # If unsafe, skip to generation (which will output crisis msg)\n",
    "\n",
    "workflow.add_conditional_edges(\"safety\", route_safety)\n",
    "\n",
    "# Normal Flow\n",
    "workflow.add_edge(\"router\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the machine\n",
    "app_graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3edd4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project - Copy\\myenv\\Lib\\site-packages\\IPython\\core\\formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project - Copy\\myenv\\Lib\\site-packages\\langgraph\\pregel\\main.py:775\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    772\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    774\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    776\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project - Copy\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:693\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config, base_url, proxies)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    684\u001b[39m     draw_mermaid_png,\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    688\u001b[39m     curve_style=curve_style,\n\u001b[32m    689\u001b[39m     node_colors=node_colors,\n\u001b[32m    690\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    691\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    692\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project - Copy\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:312\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, base_url, proxies)\u001b[39m\n\u001b[32m    306\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    307\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    308\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    309\u001b[39m         )\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project - Copy\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:475\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay, proxies, base_url)\u001b[39m\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    471\u001b[39m     msg = (\n\u001b[32m    472\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    473\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    479\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x21ca3726010>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47dfe267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shadman'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80ec32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Brain (Ollama)...\n",
      "üîå Connecting to Memory (ChromaDB)...\n",
      "   ‚úÖ Database connected.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Import M3's Persona\n",
    "from prompts import SOMY_SYSTEM_PROMPT, CRISIS_RESPONSE\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DB_PATH = \"..\\chroma_db\" # This folder must exist (From M2)\n",
    "LLM_MODEL = \"llama3.1\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "# --- 1. SETUP MODELS ---\n",
    "print(\"üîå Connecting to Brain (Ollama)...\")\n",
    "try:\n",
    "    llm = ChatOllama(model=LLM_MODEL, temperature=0.7)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "    llm = None\n",
    "\n",
    "print(\"üîå Connecting to Memory (ChromaDB)...\")\n",
    "# We connect to the DB created by M2. If it doesn't exist, we skip RAG.\n",
    "retriever = None\n",
    "if os.path.exists(DB_PATH):\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(model=EMBED_MODEL)\n",
    "        vector_store = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) # Get top 3 pages\n",
    "        print(\"   ‚úÖ Database connected.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö† Database error: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ö† WARNING: 'chroma_db' folder not found. Running in 'Amnesia Mode' (No RAG).\")\n",
    "\n",
    "# --- 2. DEFINE STATE ---\n",
    "# This dictionary holds the data as it moves through the graph nodes\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[object] # Chat history\n",
    "    context: str           # Retrieved book info\n",
    "    is_safe: bool          # Safety flag\n",
    "\n",
    "# --- 3. DEFINE NODES (The Workers) ---\n",
    "\n",
    "def safety_check(state: AgentState):\n",
    "    \"\"\"Worker 1: The Guardrail (M4 Logic)\"\"\"\n",
    "    # Get the last message from the user\n",
    "    # We check if messages list is not empty\n",
    "    if not state[\"messages\"]:\n",
    "        return {\"is_safe\": True}\n",
    "    \n",
    "    last_msg = state[\"messages\"][-1].content.lower()\n",
    "    danger_words = [\n",
    "    # Core suicide terms\n",
    "    \"suicide\",\n",
    "    \"suicidal\",\n",
    "    \"kill myself\",\n",
    "    \"end my life\",\n",
    "    \"take my own life\",\n",
    "    \"want to die\",\n",
    "    \"wish I were dead\",\n",
    "    \"death is the only solution\",\n",
    "    \"self-harm\",\n",
    "    \"self harm\",\n",
    "    \"hurt myself\",\n",
    "    \"harm myself\",\n",
    "    \"cut myself\",\n",
    "    \"cutting\",\n",
    "    \"bleed to death\",\n",
    "    \"cut my veins\",\n",
    "    \"smash my head\",\n",
    "    \"punish myself\",\n",
    "    \"overdose\",\n",
    "    \"poison myself\",\n",
    "    \"jump off\",\n",
    "    \"jump in front of a train\",\n",
    "    \"drown myself\",\n",
    "    \"suffocate myself\",\n",
    "    \"end it all\",\n",
    "    \"end the pain\",\n",
    "    \"end my suffering\",\n",
    "    \"stop existing\",\n",
    "    \"disappear forever\",\n",
    "    \"vanish from this world\",\n",
    "    \"escape life\",\n",
    "    \"free from life\",\n",
    "    \"life is meaningless\",\n",
    "    \"life is not worth living\",\n",
    "    \"no reason to live\",\n",
    "    \"no future for me\",\n",
    "    \"tired of being alive\",\n",
    "    \"done with life\",\n",
    "    \"giving up on life\",\n",
    "    \"I am a burden\",\n",
    "    \"everyone would be better without me\",\n",
    "    \"no one would miss me\",\n",
    "    \"tonight\",\n",
    "    \"right now\",\n",
    "    \"anymore\"\n",
    "]\n",
    "    \n",
    "    # Check if safe\n",
    "    is_unsafe = any(word in last_msg for word in danger_words)\n",
    "    \n",
    "    if is_unsafe:\n",
    "        print(\"üö® GUARDRAIL HIT: Unsafe content detected.\")\n",
    "        \n",
    "    return {\"is_safe\": not is_unsafe}\n",
    "\n",
    "def retrieve_knowledge(state: AgentState):\n",
    "    \"\"\"Worker 2: The Smart Librarian (Updated for Week 3)\"\"\"\n",
    "    if not retriever:\n",
    "        return {\"context\": \"No medical manuals available.\"}\n",
    "    \n",
    "    print(\"üîç Searching Knowledge Base...\")\n",
    "    user_text = state[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    # --- METADATA ROUTING LOGIC ---\n",
    "    # We check the user's text for keywords to decide which \"Book\" to open.\n",
    "    # This matches the tags M2 created in 'ingest_with_tags.py'.\n",
    "    \n",
    "    filter_dict = None # Default: Search everything\n",
    "    \n",
    "    if \"child\" in user_text or \"teen\" in user_text or \"son\" in user_text or \"daughter\" in user_text:\n",
    "        print(\"   üè∑Ô∏è Filter Applied: Audience='child'\")\n",
    "        filter_dict = {\"audience\": \"child\"}\n",
    "        \n",
    "    elif \"panic\" in user_text or \"anxiety\" in user_text or \"worry\" in user_text:\n",
    "        print(\"   üè∑Ô∏è Filter Applied: Category='anxiety'\")\n",
    "        filter_dict = {\"category\": \"anxiety\"}\n",
    "        \n",
    "    elif \"sad\" in user_text or \"depress\" in user_text or \"hopeless\" in user_text:\n",
    "        print(\"   üè∑Ô∏è Filter Applied: Category='depression'\")\n",
    "        filter_dict = {\"category\": \"depression\"}\n",
    "\n",
    "    elif \"trauma\" in user_text or \"ptsd\" in user_text or \"abuse\" in user_text:\n",
    "        print(\"   üè∑Ô∏è Filter Applied: Category='trauma'\")\n",
    "        filter_dict = {\"category\": \"trauma\"}\n",
    "    elif \"anger\" in user_text:\n",
    "        filter_dict = {\"category\": \"anger\"}\n",
    "    elif \"grief\" in user_text:\n",
    "        filter_dict = {\"category\": \"grief\"}\n",
    "    elif \"sleep\" in user_text:\n",
    "        filter_dict = {\"category\": \"sleep\"}\n",
    "    elif \"addiction\" in user_text or \"substance\" in user_text:\n",
    "        filter_dict = {\"category\": \"addiction\"}\n",
    "\n",
    "    # --- EXECUTE SEARCH ---\n",
    "    try:\n",
    "        # We pass the 'filter' to the retriever\n",
    "        if filter_dict:\n",
    "            docs = retriever.invoke(user_text, filter=filter_dict)\n",
    "        else:\n",
    "            docs = retriever.invoke(user_text) # Search everything if no keyword found\n",
    "            \n",
    "        # Combine results\n",
    "        if not docs:\n",
    "            return {\"context\": \"No specific manual found for this topic.\"}\n",
    "            \n",
    "        context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "        return {\"context\": context_text}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Retrieval Failed: {e}\")\n",
    "        return {\"context\": \"Error retrieving context.\"}\n",
    "\n",
    "# def retrieve_knowledge(state: AgentState):\n",
    "#     \"\"\"Worker 2: The Librarian (M2 Logic)\"\"\"\n",
    "#     if not retriever:\n",
    "#         return {\"context\": \"No medical manuals available.\"}\n",
    "    \n",
    "#     print(\"üîç Searching Knowledge Base...\")\n",
    "#     query = state[\"messages\"][-1].content\n",
    "#     try:\n",
    "#         docs = retriever.invoke(query)\n",
    "#         # Combine the top 3 chunks into one string\n",
    "#         context_text = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "#         return {\"context\": context_text}\n",
    "#     except Exception as e:\n",
    "#         print(f\"   ‚ö† Retrieval Failed: {e}\")\n",
    "#         return {\"context\": \"Error retrieving context.\"}\n",
    "\n",
    "def generate_response(state: AgentState):\n",
    "    \"\"\"Worker 3: The Psychologist (M3 Logic)\"\"\"\n",
    "    \n",
    "    # If safety check failed, return crisis message immediately\n",
    "    if not state[\"is_safe\"]:\n",
    "        return {\"messages\": [AIMessage(content=CRISIS_RESPONSE)]}\n",
    "    \n",
    "    if not llm:\n",
    "        return {\"messages\": [AIMessage(content=\"System Error: AI Model not connected.\")]}\n",
    "\n",
    "    print(\"ü§ñ Generating Response...\")\n",
    "    \n",
    "    # Inject the retrieved context into the System Prompt\n",
    "    # If context is empty, it just uses general knowledge\n",
    "    filled_prompt = SOMY_SYSTEM_PROMPT.format(context=state.get(\"context\", \"No manuals.\"))\n",
    "    \n",
    "    # Build the conversation history for the LLM:\n",
    "    # [System Instruction] + [User Message]\n",
    "    conversation = [SystemMessage(content=filled_prompt)] + state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(conversation)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- 4. BUILD THE GRAPH ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add the workers to the graph\n",
    "workflow.add_node(\"safety\", safety_check)\n",
    "workflow.add_node(\"retrieve\", retrieve_knowledge)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "\n",
    "# Define the Flow: Start -> Safety\n",
    "workflow.set_entry_point(\"safety\")\n",
    "\n",
    "# Define Conditional Flow (The \"Traffic Cop\")\n",
    "def route_safety(state: AgentState) -> Literal[\"retrieve\", \"generate\"]:\n",
    "    if state[\"is_safe\"]:\n",
    "        return \"retrieve\" # If safe, go get knowledge\n",
    "    else:\n",
    "        return \"generate\" # If unsafe, skip to generation (which will output crisis msg)\n",
    "\n",
    "workflow.add_conditional_edges(\"safety\", route_safety)\n",
    "\n",
    "# Normal Flow\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the machine\n",
    "app_graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8de010",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\IPython\\core\\formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langgraph\\pregel\\main.py:775\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    772\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    774\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    776\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:693\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config, base_url, proxies)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    684\u001b[39m     draw_mermaid_png,\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    688\u001b[39m     curve_style=curve_style,\n\u001b[32m    689\u001b[39m     node_colors=node_colors,\n\u001b[32m    690\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    691\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    692\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:312\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, base_url, proxies)\u001b[39m\n\u001b[32m    306\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    307\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    308\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    309\u001b[39m         )\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shaziya khan\\Desktop\\Somyali_backend_project\\myenv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:475\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay, proxies, base_url)\u001b[39m\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    471\u001b[39m     msg = (\n\u001b[32m    472\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    473\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    479\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 204.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x16d23bf0890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26682bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
